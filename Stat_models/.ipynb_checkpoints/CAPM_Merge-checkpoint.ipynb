{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d203f98-21f5-4e4b-855e-29d9f6a952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc170ffb-be17-41e4-94d3-fca1771ad889",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=\"2011-10-01\"\n",
    "end_date=\"2024-09-30\"\n",
    "window_size = 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d84f35-34c9-4d8b-a416-2b0a694f96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"ARL\",\"BH\",\"CIX\",\"FDBC\",\"GEF\",\"NATH\",\"NKSH\",\"NWFL\",\"PLBC\",\"PNRG\",\"QRTEB\",\"RGCO\",\"STRS\",\"SWKH\",\"TCI\",\"VABK\",\"VALU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571f42ba-8990-4aa7-ad11-1f342364a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['date', 'tic', 'stat_Anomaly_Probability']\n",
    "result_df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ccbc007-ab0c-41c9-9b7f-389cf642618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\hc\\AppData\\Local\\Temp\\ipykernel_11012\\1923743494.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df = pd.concat([result_df, res_df], ignore_index=True)\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "for target_ticker in tickers:\n",
    "    treasury_data =yf.download('^IRX', start=start_date, end=end_date)\n",
    "    treasury_data['daily_risk_free_rate'] = (treasury_data['Adj Close'] / 100) / 252\n",
    "    treasury_data=treasury_data[['daily_risk_free_rate']]\n",
    "    mkt_data =yf.download('^RUT', start=start_date, end=end_date)\n",
    "    mkt_data['daily_market_return'] = mkt_data['Adj Close'].pct_change()\n",
    "    mkt_data=mkt_data[['daily_market_return']]\n",
    "    stock_data =yf.download(target_ticker, start=start_date, end=end_date)\n",
    "    stock_data['daily_return'] = stock_data['Adj Close'].pct_change()\n",
    "    stock_data=stock_data[['daily_return']]\n",
    "    combined_df = pd.concat([treasury_data, mkt_data, stock_data], axis=1).dropna()\n",
    "    combined_df[\"tic\"]=target_ticker\n",
    "    combined_df[\"r_minus_rf\"]=combined_df[\"daily_return\"]-combined_df[\"daily_risk_free_rate\"]\n",
    "    combined_df[\"rm_minus_rf\"]=combined_df[\"daily_market_return\"]-combined_df[\"daily_risk_free_rate\"]\n",
    "    rolling_covariance = combined_df['rm_minus_rf'].rolling(window=window_size).cov(combined_df['r_minus_rf'])\n",
    "    rolling_variance = combined_df['rm_minus_rf'].rolling(window=window_size).var()\n",
    "    rolling_beta = rolling_covariance / rolling_variance\n",
    "    combined_df['Rolling_Beta'] = rolling_beta\n",
    "    combined_df['Excess_Return']=combined_df['r_minus_rf']-rolling_beta.shift(1)*combined_df['rm_minus_rf']\n",
    "    combined_df=combined_df.dropna()\n",
    "    combined_df['rolling_mean']= combined_df['Excess_Return'].rolling(window=window_size).mean()\n",
    "    combined_df['rolling_std_dev']= combined_df['Excess_Return'].rolling(window=window_size).std()\n",
    "    combined_df['ratio_to_std']= combined_df['Excess_Return']/combined_df['rolling_std_dev']\n",
    "    combined_df['stat_Anomaly_Probability']= 2*np.abs(norm.cdf(combined_df['ratio_to_std'], 0, 1)-0.5)\n",
    "    combined_df = combined_df.dropna()\n",
    "    combined_df_reset = combined_df.reset_index()\n",
    "    combined_df_reset.rename(columns={'Date':'date'}, inplace=True)\n",
    "    res_df=combined_df_reset[['date','tic','stat_Anomaly_Probability']]\n",
    "    result_df = pd.concat([result_df, res_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2723cadb-23c6-48be-9bc2-8a156a2d0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('df_final_prob.csv', low_memory=False)\n",
    "data2 = data2[['date','tic','close','volume','DBSCAN_Anomaly_Probability','IsolationForest_Anomaly_Probability','OCSVM_Anomaly_Probability','LSTM_Anomaly_Probability']]\n",
    "data2['date'] = pd.to_datetime(data2['date'], format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13d9484-aab3-4f7d-82ab-08d0dc43f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(data2, result_df, on=['date', 'tic'],how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2be61d1-8e28-4bbd-9cb4-b71e171aa030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('df_final_prob_renewed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37a013-422e-40f8-bc89-69f69668f402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
